\section{Разработка программы-симулятора} % (fold)
\label{sec:MotivationForProgramm}
    Как ясно видно из предыдущей главы, основным средством для исследования вопросов коллективного движения являются численные эксперименты. В первую очередь это связано с особенностями формирования групп - обычно они достаточно маленькие, чтобы было возможно вычислить точное (в рамках ошибки округления) положение всех обьектов произведя простую дискретизацию времени, так же как описано в алгоритме Вичека. Это существенно отличает проблему коллективного движения от обычных задач статистической механики или конкретно гидродинамики, в которых фигурирует число частиц на несколько порядков превышающее совокупную емкость памяти всех компьютеров на Земле. Другим отличием самодвижущихся частиц от обычных является наличие не вполне обычного взаимодействия, которое приводит к возникновению дальнего порядка связи []. Это, конечно, осложняет вычисления, поскольку необходимо убедиться, с одной стороны, в том что в системе прошло достаточно ``времени'' для релаксации, а с другой - в том, что за это время не накопились значительные ошибки округления чисел с плавающей запятой. \marginpar{сравнения double \& float}

    С точки зрения реализации в виде компьютерной программы, алгоритм, предложенный Вичеком и рассмотренный в \ref{sec:CompModelsBasics}, а также аналоги, рассмотренные в \ref{sec:VariantsOfVicsekModel} обладает несколькими явно выраженными достоинствами, связанными с архитектурой современных компьютеров и вычислительных центров.
    \subsection{Особенности многопоточной обработки данных} % (fold)
    \label{sub:MultithreadMulticoreDataProcessing}
        В течении десятилетий с момента создания интегральной микросхемы и вычислительных устройств на ее основе, развитие компьютеров определяется законом Мура, а совершенствование технологических процессов позволяет надеяться что рано или поздно будут созданы компьютеры превосходящие по вычислительной мощности любой из доступных в каждый конкретный момент времени. Однако несмотря на то, что количество транзисторов в микропроцессоре с каждым годом увеличивается по экспоненциальному закону, это лишь опосредствовано приводит к увеличению производительности. Не вдаваясь в детали, отметим, что вплоть до начала двухтысячных годов производители микропроцессоров общего назначения совершенствовали паралеллизм на уровне инструкций, что позволяло получить улучшение производительности программ безо всяких усилий со стороны разработчика, или с минимумом усилий, затраченных на перекомпиляцию. Однако, уменьшение технопроцесса с целью увеличения числа транзисторов на кристалле привело увеличению токов утечки, а значит и к повышению потребляемой мощности ($\sim 100$ Вт) и соответственно тепловыделения процессоров. Потому, вместо усложнения одного ядра процессора современные процессоры общего назначения состоят из нескольких (от двух до восьми) ядер, каждое из которых является обособленным процессором, но при этом связано общей шиной данных высокой частоты с окружающими ядрами, и возможно обладает поддержкой гиперпоточности, что позволяет довести общее число ``одновременно'' исполняемых процессов до 30 [].

        То, что сейчас происходит в сегменте потребительских процессоров общего назначения, уже давно произошло в сегменте высоконагруженных серверных систем, и систем обработки больших обьемов данных (т.н. суперкомпьютеров). Поскольку суперкомпьютеры изначально создавались из рассчета на обработку данных, с которыми не способны были справиться одиночные коспьютеры, в них имеют место несколько архитектурных особенностей, главной из которох является распределенность вычислений.

        Как известно, едва ли не больше, чем скорость операций, выполняемых процессором, на производительность вычислений влияет скорость получения данных и инструкций из памяти. И не смотря на совершенствование технологий и значительный прогресс, достигнутый за последние десятилетия, оперативная память все еще остается узким горлышком традиционной архитектуры. До широкого распространения архитектур просессоров с 64 разрядной адрессацией памяти еще одной проблемой была невозможность использовать одновременно больше 4-х гигабайт ($2^{32}$) оперативной памяти. Несмотртя на то что в физических приложениях такое количество данных встречается не слишком часто, катастрофическое превосходство во времени доступа к кеш-памяти процессора над скоростью доступа к оперативной памяти играет здесь немалую роль. Если же взглянуть на архитектуру суперкомпьютера [], то ясно видно, что возникает еще один слой памяти, общий для всех процессоров, доступ к которому еще медленнее, чем к оперативной памяти.

        Однако, помимо процессоров общего назначения с несколькими (десятками) ядер, на рынке существуют и вполне доступны процессоры специального назначения. Изначально это были арифметические модули для выполнения арифметических действия с числами с плавающей запятой, потом они частично были включены в общее ядро с CPU, частично эволюционировали в ``заточенные'' под вычисление определенных функций модули. И нишу специальных, но совершенно необходимых устройств заняли процессоры для работы с графикой (GPU), широко известные под названием ``видеокарты''.

        Основными отличиями видеокарт от процессоров общего назначения являются следующие:
        \begin{itemize}
            \item они обладают большим количеством потоковых процессоров, которые могут выполнять простейшие арифметические действия
            \item с другой стороны, они обладают ограниченным набором инструкций
            \item но при этом даже общая графическая память быстрее на чтение, чем оперативная память
            \item процессоры обьединены в блоки с общей (более быстрой) памятью
            \item в следствие оптимизации для работы с графикой, манипулирование 4-х векторами чисел очень эффективно
            \item по той-же причине наиболее эффективны вычисления с числами одинарной точности
        \end{itemize}
        В течении последних нескольких лет происходит массовая замена CPU на GPU в высокопроизводительных системах, особенно входящих в TOP 500 суперкомпьютеров мира. [] Помимо особенностей в организации памяти, главным преимуществом GPU по сравнению с CPU является стоимость в пересчете на один поток вычислений, затем следует производительность на ватт, вычисляемая с условием эквивалентной производительности системы из CPU и из GPU.

        В контексте проблемы компьютерной реализации модели Вичека, следует обратить внимание на несколько фактов:
        \begin{itemize}
            \item Модель полностью детерменирована в том смысле, что информация о предыдущем положении частиц однозначно определяет состояние системы на текущем шаге по времени до наложения шума.
            \item Поведение системы определяется локальным взаимодействием, что позволяет значительно ускорить обработку данных, поместив соседние частицы в один ``банк'' памяти.
            \item Простота модели позволяет реализовать ее без значительных затрат усилий для вычисления на GPU
        \end{itemize}
        Все вышесказанное приводит нас к выводу, что без учета взаимодействия с памятью, скорость вычисления модели Вичека должна расти линейно относительно количества потоков, пока последняя не дойдет до числа частиц. С другой стороны, на синхронизацию состояния каждой копии данных в индивидуальных адресных пространствах потоков будет затрачено время, линейное относительно числа потоков. Хранение всех данных в общем адресном пространстве хоть и снимает необходимость в синхронизации, но при этом увеличивает время выполнения каждого вычисления из-за низкой скорости доступа к данным. Следовательно, необходим разумный компромис между количеством данных для синхронизации, и потенциальным выигрышем в скорости доступа.

    % subsection MultithreadMulticoreDataProcessing (end)

    \subsection{Задачи перед программой} % (fold)
    \label{sub:TaksForProgramm}
        Несмотря на существование большого числа авторов, которые проводили симуляции по модели Вичека (см. пункт \ref{sec:CompModelsBasics}), доступность наработок для широкой общественности оставляет желать лучшего. Кроме того, по косвенным свидетельствам можно судить, что разработанные приложения в первую очередь ориентированы на выполнение на мультипроцессорных кластерах, что, по понятным причинам, не подходит к нашим реалиям. Можно предположить что лишь в исследовании, проведенном Алдано и соавторами \cite{baglietto2008,aldana2009} фигурировали GPU (число частиц кратно 512), но явно это не указано. В любом случае, общеизвестно, что наилучшее обучение - это практика, и потому было принято решение реализовать с нуля программу, реализиющую модель Вичека.

        Поскольку перед нами не стоял технический долг, была возможность заложить в архитектуру программы новейшие наработки в области многопоточного программирования и гетерогенных вычислений. Помимо этого в архитектуру программы был заложен потенциал к дальнейшему наращиванию функционала (а именно - расширению базы используемых моделей) без переписывания (и даже перекомпиляции) ядра, ответственного за организацию работы GPU. Косвенно был адресован вопрос оптимизации программы для имеющегося в нашем распоряжении железа.

        Помимо архитектурных задач, связанных сразработкой ПО, была поставлена конкретная физическая задача: получение профилей скорости в Куэтт-подобном течении.
    % subsection TaksForProgramm (end)
% section MotivationForProgramm (end)