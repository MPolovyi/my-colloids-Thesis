\section{Разработка программы-симулятора} % (fold)
\label{sec:MotivationForProgramm}
    Как ясно видно из предыдущей главы, основным средством для исследования вопросов коллективного движения являются численные эксперименты. В первую очередь это связано с особенностями формирования групп - обычно они достаточно маленькие, чтобы было возможно вычислить точное (в рамках ошибки округления) положение всех обьектов произведя простую дискретизацию времени, так же как описано в алгоритме Вичека. Это существенно отличает проблему коллективного движения от обычных задач статистической механики или конкретно гидродинамики, в которых фигурирует число частиц на несколько порядков превышающее совокупную емкость памяти всех компьютеров на Земле. Другим отличием самодвижущихся частиц от обычных является наличие не вполне обычного взаимодействия, которое приводит к возникновению дальнего порядка связи []. Это, конечно, осложняет вычисления, поскольку необходимо убедиться, с одной стороны, в том что в системе прошло достаточно ``времени'' для релаксации, а с другой - в том, что за это время не накопились значительные ошибки округления чисел с плавающей запятой. \marginpar{сравнения double \& float}

    С точки зрения реализации в виде компьютерной программы, алгоритм, предложенный Вичеком и рассмотренный в \ref{sub:CompModelsBasics}, а также аналоги, рассмотренные в \ref{sub:VariantsOfVicsekModel} обладает несколькими явно выраженными достоинствами, связанными с архитектурой современных компьютеров и вычислительных центров.
    \subsection{Особенности многопоточной обработки данных} % (fold)
    \label{sub:MultithreadMulticoreDataProcessing}
        В течении десятилетий с момента создания интегральной микросхемы и вычислительных устройств на ее основе, развитие компьютеров определяется законом Мура, а совершенствование технологических процессов позволяет надеяться что рано или поздно будут созданы компьютеры превосходящие по вычислительной мощности любой из доступных в каждый конкретный момент времени. Однако несмотря на то, что количество транзисторов в микропроцессоре с каждым годом увеличивается по экспоненциальному закону, это лишь опосредствовано приводит к увеличению производительности. Не вдаваясь в детали, отметим, что вплоть до начала двухтысячных годов производители микропроцессоров общего назначения совершенствовали паралеллизм на уровне инструкций, что позволяло получить улучшение производительности программ безо всяких усилий со стороны разработчика, или с минимумом усилий, затраченных на перекомпиляцию. Однако, уменьшение технопроцесса с целью увеличения числа транзисторов на кристалле привело увеличению токов утечки, а значит и к повышению потребляемой мощности ($\sim 100 Вт$) и соответственно тепловыделения процессоров. Потому, вместо усложнения одного ядра процессора современные процессоры общего назначения состоят из нескольких (от двух до восьми) ядер, каждое из которых является обособленным процессором, но при этом связано общей шиной данных высокой частоты с окружающими ядрами, и возможно обладает поддержкой гиперпоточности, что позволяет довести общее число ``одновременно'' исполняемых процессов до 30 [].

        То, что сейчас происходит в сегменте потребительских процессоров общего назначения, уже давно произошло в сегменте высоконагруженных серверных систем, и систем обработки больших обьемов данных (т.н. суперкомпьютеров). Поскольку суперкомпьютеры изначально создавались из рассчета на обработку данных, с которыми не способны были справиться одиночные коспьютеры, в них имеют место несколько архитектурных особенностей, главной из которох является распределенность вычислений.

        Как известно, едва ли не больше, чем скорость операций, выполняемых процессором, на производительность вычислений влияет скорость получения данных и инструкций из памяти. И не смотря на совершенствование технологий и значительный прогресс, достигнутый за последние десятилетия, оперативная память все еще остается узким горлышком традиционной архитектуры. До широкого распространения архитектур просессоров с 64 разрядной адрессацией памяти еще одной проблемой была невозможность использовать одновременно больше 4-х гигабайт ($2^(32)$) оперативной памяти. Несмотртя на то что в физических приложениях такое количество данных встречается не слишком часто, катастрофическое превосходство во времени доступа к кеш-памяти процессора над скоростью доступа к оперативной памяти играет здесь немалую роль. Если же взглянуть на архитектуру суперкомпьютера [], то ясно видно, что возникает еще один слой памяти, общий для всех процессоров, доступ к которому еще медленнее, чем к оперативной памяти.

        Однако, помимо процессоров общего назначения с несколькими (десятками) ядер, на рынке существуют и вполне доступны процессоры специального назначения. Изначально это были арифметические модули для выполнения арифметических действия с числами с плавающей запятой, потом они частично были включены в общее ядро с CPU, частично эволюционировали в ``заточенные'' под вычисление определенных функций модули. И нишу специальных, но совершенно необходимых устройств заняли процессоры для работы с графикой (GPU), широко известные под названием ``видеокарты''.

        Основными отличиями видеокарт от процессоров общего назначения являются следующие:
        \begin{itemize}
            \item они обладают большим количеством потоковых процессоров, которые могут выполнять простейшие арифметические действия
            \item с другой стороны, они обладают ограниченным набором инструкций
            \item но при этом даже общая графическая память быстрее на чтение, чем оперативная память
            \item процессоры обьединены в блоки с общей (более быстрой) памятью
            \item в следствие оптимизации для работы с графикой, манипулирование 4-х векторами чисел очень эффективно
            \item по той-же причине наиболее эффективны вычисления с числами одинарной точности
        \end{itemize}
        В течении последних нескольких лет происходит массовая замена CPU на GPU в высокопроизводительных системах, особенно входящих в TOP 500 суперкомпьютеров мира. [] Помимо особенностей в организации памяти, главным преимуществом GPU по сравнению с CPU является стоимость в пересчете на один поток вычислений, затем следует производительность на ватт, вычисляемая с условием эквивалентной производительности системы из CPU и из GPU.

        В контексте проблемы компьютерной реализации модели Вичека, следует обратить внимание на несколько фактов:
        \begin{itemize}
            \item Модель полностью детерменирована в том смысле, что информация о предыдущем положении частиц однозначно определяет состояние системы на текущем шаге по времени до наложения шума.
            \item Поведение системы определяется локальным взаимодействием, что позволяет значительно ускорить обработку данных, поместив соседние частицы в один ``банк'' памяти.
            \item Простота модели позволяет реализовать ее без значительных затрат усилий для вычисления на GPU
        \end{itemize}
        Все вышесказанное приводит нас к выводу, что без учета взаимодействия с памятью, скорость вычисления модели Вичека должна расти линейно относительно количества потоков, пока последняя не дойдет до числа частиц. С другой стороны, на синхронизацию состояния каждой копии данных в индивидуальных адресных пространствах потоков будет затрачено время, линейное относительно числа потоков. Хранение всех данных в общем адресном пространстве хоть и снимает необходимость в синхронизации, но при этом увеличивает время выполнения каждого вычисления из-за низкой скорости доступа к данным. Следовательно, необходим разумный компромис между количеством данных для синхронизации, и потенциальным выигрышем в скорости доступа.

    % subsection MultithreadMulticoreDataProcessing (end)

    \subsection{Задачи перед программой} % (fold)
    \label{sub:TaksForProgramm}
        Несмотря на существование большого числа авторов, которые проводили симуляции по модели Вичека (см. пункт \ref{sub:CompModelsBasics}), доступность наработок для широкой общественности оставляет желать лучшего. Кроме того, по косвенным свидетельствам можно судить, что разработанные приложения в первую очередь ориентированы на выполнение на мультипроцессорных кластерах, что, по понятным причинам, не подходит к нашим реалиям. Можно предположить что лишь в исследовании, проведенном Алдано и соавторами \cite{baglietto2008,aldana2009} фигурировали GPU (число частиц кратно 512), но явно это не указано. В любом случае, общеизвестно, что наилучшее обучение - это практика, и потому было принято решение реализовать с нуля программу, реализиющую модель Вичека.

        Поскольку перед нами не стоял технический долг, была возможность заложить в архитектуру программы новейшие наработки в области многопоточного программирования и гетерогенных вычислений. Помимо этого в архитектуру программы был заложен потенциал к дальнейшему наращиванию функционала (а именно - расширению базы используемых моделей) без переписывания (и даже перекомпиляции) ядра, ответственного за организацию работы GPU. Косвенно был адресован вопрос оптимизации программы для имеющегося в нашем распоряжении железа.

        Помимо архитектурных задач, связанных сразработкой ПО, была поставлена конкретная физическая задача: получение профилей скорости в Куэтт-подобном течении.
    % subsection TaksForProgramm (end)


    \subsection{Описание программы, моделирующей модель Вичека} % (fold)
    \label{sub:ViksecModelProgramm}
        Итак, как должно быть понятно из пункта \ref{sub:MultithreadMulticoreDataProcessing}, мы полагаем, что модель Вичека, а конкретно ее особенности связанные с локальностью взаимодействия, и архитектура GPU отлично подходят друг для друга.

        Наконец, алгоритм вычисления в нашем случае выглядит следующим образом: []
        Поскольку задача оптимизации вычислений ставилась лишь косвенно, мы не экспериментировали с тем, что касается разбиения задачи на потоки, и с частотой сортировки информации в памяти, и возможно было бы достичь лучшей оптимизации.
        
        Однако, предложено использовать простой критерий релаксации системы. Как известно из термодинамики, влияние флуктуаций на параметры системы пропорционально $1/\sqrt{N}$, где $N$ - число частиц. Потому мы предложили выполнять для начала 100 шагов по времени для каждого значения шума, а затем, если разница средней скорости и средней скорости на предыдущей итерации \textit{НЕ} оказывается меньше чем $1/\sqrt{N}$, удваивать число шагов, и продолжать симуляцию с тем же значением шума (см. алгоритм на рис. [])\marginpar{нарисовать алгоритм!}
    % subsection ViksecModelProgramm (end)

    \subsection{Получение профилей скорости Куэтт-подобного потока} % (fold)
    \label{sub:SpeedProfilesOfCouetteFlow}
        Как было сказано в разделе \ref{sec:TheoreticalModels}, посвященном теоретическим моделям самодвижущихся частиц, признанные широкой общественностью модели самодвижущихся частиц на данный момент так или иначе включают в себя вязкость. Однако не сложно заметить, что в модель Вичека, ввиду отсутствия в ней взаимодействия между частицами, отличного от упорядочивания направления движения, не предполагает наличия между частицами вязких сил в привычном понимании. Известно, что простейшей задачей связанной с течением вязкой жидкости, допускающей к тому-же аналитическое решение, является течение Куэтта. В таком случае установившаяся скорость потока в зависимости от высоты описывается следующим выражением:
        \begin{equation} \label{eq:CoetteFlow}
            u(y) = u_0 \frac{y}{h} + \frac{1}{2\mu} + (\frac{dp}{dx}(y^2 -hy))
        \end{equation}
        где $\mu$ - вязкость жидкости, $\frac{dp}{dx}$ - градиент давления. Результирующие профили скорости представлены на рис [].
        Таким образом, для обоснования применимости теории, представленной в \cite{chepizhko2013} и рассмотренной в пункте \ref{sub:KulinskyModel}, достаточно показать, что установившиеся профили скорости в Куэтт-подобной задаче для самодвижущихся частиц никоим образом не соотносятся с профилями течения Куэтта вязкой жидкости.

        Задача Куэтта в классическом понимании ставится следующим образом: вязкая ньютоновская жидкость, заключенна между двумя бесконечными горизонтальными плоскостями на расстоянии $h$ одна от другой, нижняя пластина покоится (в силу вязкости скорость жидкости $u(0) = 0$), в то время как верхняя движется в направлении $x$ со скоростью $u_0$ (аналогично, в силу вязкости скорость жидкости $u(h) = u_0$). Необходимо найти зависимость скорости жидкости от высоты.

        Поскольку мы постулируем отсутствие вязкости, и тем более потому что речь идет о самодвижущихся частицах, нобходимо несколько модифицировать граничные условия. Для начала, рассматривается прямоугольная область, на вертикальных стенках которой заданы периодические граничные условия. Потом, нижняя стенка принята отражающей, что конечно не совсем верно и приводит к некоторым особенностям полученных профилей. (см. раздел \ref{sub:ProgrammResults}). На верхней границе принято следующее: что бы не нарушать допущение об отсутствии взимодействий кроме меняющих направление, верхняя граница учитывается как дополнительное ``скопление частиц'', движущееся в заданном направлении, и учитывается после учета взаимодействия частицы с окружением, в том случае если расстояние до нее оказывается меньше радуиса взаимодействия частиц.
    % subsection SpeedProfilesOfCouetteFlow (end)
% section MotivationForProgramm (end)